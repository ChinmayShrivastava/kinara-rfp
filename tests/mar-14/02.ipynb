{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chinmayshrivastava/Documents/GitHub/kinara-rfp\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# moce two directories up and make that current directory\n",
    "os.chdir(\"../..\")\n",
    "# print current directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from govrfp.rfpinsights._chromadb import return_collection, add_entries_to_collection\n",
    "path = \"tests/mar-14/data/\"\n",
    "critical_collection = return_collection(path=path, collection_name=\"critical\")\n",
    "informative_collection = return_collection(path=path, collection_name=\"informative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, 115, 188)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_data_path = \"tests/mar-14/02-critical-compliances.txt\"\n",
    "informative_data_path = \"tests/mar-14/02-informative-compliances.txt\"\n",
    "# open both and read all lines separately\n",
    "with open(critical_data_path, 'r') as file:\n",
    "    critical_data = file.readlines()\n",
    "with open(informative_data_path, 'r') as file:\n",
    "    informative_data = file.readlines()\n",
    "type(critical_data), type(informative_data), len(critical_data), len(informative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 188)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_entries_to_collection(\n",
    "    docs=critical_data,\n",
    "    metadata=[{\"id\": str(i)} for i in range(len(critical_data))],\n",
    "    ids = [str(i) for i in range(len(critical_data))],\n",
    "    collection=critical_collection\n",
    ")\n",
    "add_entries_to_collection(\n",
    "    docs=informative_data,\n",
    "    metadata=[{\"id\": str(i)} for i in range(len(informative_data))],\n",
    "    ids = [str(i) for i in range(len(informative_data))],\n",
    "    collection=informative_collection\n",
    ")\n",
    "critical_collection.count(), informative_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Strength - is an attribute of the Offeror's proposal that exceeds the specified performance or capability requirements in a way that is beneficial to the government.\",\n",
       " 'Weakness - is a flaw in the proposal that increases the risk of unsuccessful contract performance',\n",
       " 'Significant Weakness - in the proposal is a flaw that appreciably increases the risk of unsuccessful contract performance.',\n",
       " 'Deficiency - is a material failure of a proposal to meet a Government requirement or a combination of significant weaknesses in a proposal that increases the risk of unsuccessful contract performance to an unacceptable level',\n",
       " 'A proposal with a deficiency cannot be found eligible for award and is rated “unacceptable”. Proposals rated that have 1 or more deficiencies for any of the Factors are NOT eligible for award and will be excluded from further consideration.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path = \"tests/mar-14/RFP_Factors_and_Rating_Criteria_-_Phase 2_.txt\"\n",
    "with open(test_data_path, 'r') as file:\n",
    "    test_data = file.readlines()\n",
    "test_data = [line.strip() for line in test_data]\n",
    "test_data = [line for line in test_data if line]\n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the given CRITICAL compliances, and INFORMATIVE compliances, return 1 for CRITICAL and 0 for INFORMATIVE for each of the user query based on what type of compliance it is.Return None if the user query isn't found in either of the compliances.CRITICAL compliances:\n",
      "{critical}\n",
      "INFORMATIVE compliances:\n",
      "{informative}\n",
      "User Query: {query}\n",
      "Return 0, 1 or None based on the type of compliance the user query is.\n",
      "Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"From the given CRITICAL compliances, and INFORMATIVE compliances, return 1 for CRITICAL and 0 for INFORMATIVE for each of the user query based on what type of compliance it is.\"\n",
    "    \"Return None if the user query isn't found in either of the compliances.\"\n",
    "    \"CRITICAL compliances:\\n\"\n",
    "    \"{critical}\\n\"\n",
    "    \"INFORMATIVE compliances:\\n\"\n",
    "    \"{informative}\\n\"\n",
    "    \"User Query: {query}\\n\"\n",
    "    \"Return 0, 1 or None based on the type of compliance the user query is.\\n\"\n",
    "    \"Response:\\n\"\n",
    ")\n",
    "print(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(query, collection, top_n= 15):\n",
    "    results = collection.query(query_texts=[query], n_results=top_n)\n",
    "    docs = results['documents'][0]\n",
    "    return \"\\n\".join([f\"{i+1}. {doc}\" for i, doc in enumerate(docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:42<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "ratings = []\n",
    "for query in tqdm.tqdm(test_data):\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        critical=get_top_n(query, critical_collection),\n",
    "        informative=get_top_n(query, informative_collection),\n",
    "        query=query\n",
    "    )\n",
    "    response = llm.complete(prompt).text\n",
    "    ratings.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of none\n",
    "none_count = sum([1 for rating in ratings if 'none' in rating.lower()])\n",
    "none_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of 1\n",
    "one_count = sum([1 for rating in ratings if '1' in rating])\n",
    "one_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of 0\n",
    "zero_count = sum([1 for rating in ratings if '0' in rating])\n",
    "zero_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
